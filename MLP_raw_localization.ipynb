{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUrp62pHSuCN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8-RADcjZK1y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "usa-FPRMa0n7"
   },
   "outputs": [],
   "source": [
    "def create_data(X, y):  \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\n",
    "    return X_train, X_valid, y_train, y_valid  \n",
    "\n",
    "def create_datasets(X1, X2, X3, X4, X5, X6, X7, X8, X9, y):\n",
    "    X1 = torch.tensor(X1, dtype=torch.float32) \n",
    "    X2 = torch.tensor(X2, dtype=torch.float32) \n",
    "    X3 = torch.tensor(X3, dtype=torch.float32) \n",
    "    X4 = torch.tensor(X4, dtype=torch.float32) \n",
    "    X5 = torch.tensor(X5, dtype=torch.float32) \n",
    "    X6 = torch.tensor(X6, dtype=torch.float32)     \n",
    "    X7 = torch.tensor(X7, dtype=torch.float32) \n",
    "    X8 = torch.tensor(X8, dtype=torch.float32) \n",
    "    X9 = torch.tensor(X9, dtype=torch.float32)     \n",
    "    \n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    data_ds = TensorDataset(X1, X2, X3, X4, X5, X6, X7, X8, X9, y)\n",
    "    return data_ds\n",
    "\n",
    "def create_dataloaders(train_ds, valid_ds, bs=128):\n",
    "    train_dl = DataLoader(train_ds, bs, shuffle=True)\n",
    "    valid_dl = DataLoader(valid_ds, bs, shuffle=False)\n",
    "    return train_dl, valid_dl  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h7EZsB4ya0kf"
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Sequential(nn.Linear(input_dim,1000), nn.ReLU())\n",
    "        self.fc2 = nn.Sequential(nn.Linear(1000,100), nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x     \n",
    "\n",
    "\n",
    "class MyEnsemble(nn.Module):\n",
    "    def __init__(self, MLPClassifier, output_dim):\n",
    "        super(MyEnsemble, self).__init__()        \n",
    "        self.MLP = MLPClassifier  \n",
    "        self.fc = nn.Sequential(nn.Linear(900,100), nn.ReLU())\n",
    "        self.classifier = nn.Sequential(nn.Linear(100, output_dim), nn.Softmax(dim=1))\n",
    "        \n",
    "    def forward(self, x1, x2, x3, x4, x5, x6, x7, x8, x9):\n",
    "        x1 = self.MLP(x1)\n",
    "        x2 = self.MLP(x2)\n",
    "        x3 = self.MLP(x3)\n",
    "        x4 = self.MLP(x4)\n",
    "        x5 = self.MLP(x5)\n",
    "        x6 = self.MLP(x6)\n",
    "        x7 = self.MLP(x7)\n",
    "        x8 = self.MLP(x8)\n",
    "        x9 = self.MLP(x9)        \n",
    "        x = torch.cat((x1, x2, x3, x4, x5, x6, x7, x8, x9), dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms(y):\n",
    "    return np.sqrt(np.mean(y**2))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRLY4aryS8v7"
   },
   "source": [
    "**DATA PREPARATION**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V9BxLbnV9hAo"
   },
   "outputs": [],
   "source": [
    "work = []\n",
    "signal = []\n",
    "list_file = ['time_serie_1.p', 'time_serie_2.p', 'time_serie_3.p', 'time_serie_4.p', 'time_serie_5.p', 'time_serie_6.p']\n",
    "\n",
    "for file in list_file:\n",
    "  with open(file,'rb') as f:\n",
    "        while True:\n",
    "          try:\n",
    "            u = pickle._Unpickler(f)\n",
    "            u.encoding = 'latin1' \n",
    "            p = u.load()      \n",
    "            A = p['acc']\n",
    "            signal.append(A)\n",
    "            work.append(p['jobname'])\n",
    "\n",
    "          except EOFError:\n",
    "            break            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('compare_single.csv')\n",
    "s = df['label_l'].to_list()\n",
    "job = df['jobname'].to_list()\n",
    "\n",
    "sensor = [2,5,68,11,14,17,20,23,26]\n",
    "label = {}\n",
    "for i in range(len(job)):\n",
    "    lab = -1\n",
    "    if s[i] in sensor:\n",
    "        lab = s[i]\n",
    "\n",
    "    label[int(job[i])] = int(lab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in range(len(work)):\n",
    "    y.append(label[int(work[i])])\n",
    "\n",
    "y = np.array(y)    \n",
    "signal0 = np.array(signal)[:,:6000,:]\n",
    "\n",
    "list_id = np.where(y!=-1)\n",
    "\n",
    "signal0 = signal0[list_id]\n",
    "y = y[list_id]\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(np.unique(y))\n",
    "y = le.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIG-Fi71FvWg"
   },
   "outputs": [],
   "source": [
    "Xmax = np.max(signal0)\n",
    "Xmin = np.min(signal0)\n",
    "signal0 = (signal0 - Xmin) / (Xmax-Xmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_rms= rms(signal0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = signal0\n",
    "yy = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal1 = signal0 + 0.001*v_rms*np.random.normal(0.0, 1.0, np.shape(signal0))\n",
    "signal2 = signal0 + 0.001*v_rms*np.random.normal(0.0, 1.0, np.shape(signal0))\n",
    "signal3 = signal0 + 0.001*v_rms*np.random.normal(0.0, 1.0, np.shape(signal0))\n",
    "signal4 = signal0 + 0.001*v_rms*np.random.normal(0.0, 1.0, np.shape(signal0))\n",
    "signal5 = signal0 + 0.001*v_rms*np.random.normal(0.0, 1.0, np.shape(signal0))\n",
    "signal6 = signal0 + 0.001*v_rms*np.random.normal(0.0, 1.0, np.shape(signal0))\n",
    "signal7 = signal0 + 0.001*v_rms*np.random.normal(0.0, 1.0, np.shape(signal0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.concatenate((signal0, signal1, signal2, signal3),axis=0)\n",
    "X = np.concatenate((signal0, signal1, signal2, signal3, signal4, signal5, signal6, signal7),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yy = np.concatenate((y,y,y,y),axis=0)\n",
    "yy = np.concatenate((y,y,y,y,y,y,y,y),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5544, 6000, 9) (5544,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X), np.shape(yy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DUhGhuGukH1g"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = create_data(X, yy)\n",
    "\n",
    "X1_train = X_train[:,:,0] \n",
    "X2_train = X_train[:,:,1] \n",
    "X3_train = X_train[:,:,2] \n",
    "X4_train = X_train[:,:,3] \n",
    "X5_train = X_train[:,:,4] \n",
    "X6_train = X_train[:,:,5] \n",
    "X7_train = X_train[:,:,6] \n",
    "X8_train = X_train[:,:,7] \n",
    "X9_train = X_train[:,:,8] \n",
    "\n",
    "X1_valid = X_valid[:,:,0] \n",
    "X2_valid = X_valid[:,:,1] \n",
    "X3_valid = X_valid[:,:,2] \n",
    "X4_valid = X_valid[:,:,3] \n",
    "X5_valid = X_valid[:,:,4] \n",
    "X6_valid = X_valid[:,:,5] \n",
    "X7_valid = X_valid[:,:,6] \n",
    "X8_valid = X_valid[:,:,7] \n",
    "X9_valid = X_valid[:,:,8] \n",
    "\n",
    "train_ds = create_datasets(X1_train, X2_train, X3_train, X4_train, X5_train, X6_train, X7_train, X8_train, X9_train, y_train)\n",
    "valid_ds = create_datasets(X1_valid, X2_valid, X3_valid, X4_valid, X5_valid, X6_valid, X7_valid, X8_valid, X9_valid, y_valid)\n",
    "train_dl, valid_dl = create_dataloaders(train_ds, valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wo7HpPPXl7I7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   3. Loss: 2.0860. Acc.: 17.12%\n",
      "Epoch:   6. Loss: 2.0798. Acc.: 17.12%\n",
      "Epoch:   9. Loss: 2.0624. Acc.: 17.12%\n",
      "Epoch:  12. Loss: 2.0747. Acc.: 17.12%\n",
      "Epoch:  15. Loss: 2.0711. Acc.: 17.12%\n",
      "Epoch:  18. Loss: 2.0843. Acc.: 17.12%\n",
      "Epoch:  21. Loss: 2.0870. Acc.: 17.12%\n",
      "Epoch:  24. Loss: 2.0759. Acc.: 17.12%\n",
      "Epoch:  27. Loss: 2.0771. Acc.: 18.02%\n",
      "Epoch:  30. Loss: 2.0776. Acc.: 17.30%\n",
      "Epoch:  33. Loss: 2.0459. Acc.: 23.06%\n",
      "Epoch:  36. Loss: 2.0328. Acc.: 27.03%\n",
      "Epoch:  39. Loss: 1.9968. Acc.: 32.97%\n",
      "Epoch:  42. Loss: 1.9265. Acc.: 33.51%\n",
      "Epoch:  45. Loss: 1.9288. Acc.: 36.94%\n",
      "Epoch:  48. Loss: 1.9286. Acc.: 36.58%\n",
      "Epoch:  51. Loss: 1.9085. Acc.: 37.30%\n",
      "Epoch:  54. Loss: 1.8620. Acc.: 44.14%\n",
      "Epoch:  57. Loss: 1.9125. Acc.: 45.23%\n",
      "Epoch:  60. Loss: 1.7566. Acc.: 46.31%\n",
      "Epoch:  63. Loss: 1.7776. Acc.: 50.27%\n",
      "Epoch:  66. Loss: 1.7866. Acc.: 50.63%\n",
      "Epoch:  69. Loss: 1.8014. Acc.: 52.07%\n",
      "Epoch:  72. Loss: 1.7639. Acc.: 52.79%\n",
      "Epoch:  75. Loss: 1.7244. Acc.: 56.22%\n",
      "Epoch:  78. Loss: 1.5860. Acc.: 60.00%\n",
      "Epoch:  81. Loss: 1.6369. Acc.: 63.60%\n",
      "Epoch:  84. Loss: 1.5764. Acc.: 64.14%\n",
      "Epoch:  87. Loss: 1.6428. Acc.: 64.86%\n",
      "Epoch:  90. Loss: 1.5502. Acc.: 65.59%\n",
      "Epoch:  93. Loss: 1.5728. Acc.: 65.59%\n",
      "Epoch:  96. Loss: 1.5756. Acc.: 65.59%\n",
      "Epoch:  99. Loss: 1.5700. Acc.: 66.13%\n",
      "Epoch: 102. Loss: 1.6909. Acc.: 66.13%\n",
      "Epoch: 105. Loss: 1.5788. Acc.: 66.49%\n",
      "Epoch: 108. Loss: 1.5323. Acc.: 66.31%\n",
      "Epoch: 111. Loss: 1.6719. Acc.: 66.31%\n",
      "Epoch: 114. Loss: 1.5127. Acc.: 66.49%\n",
      "Epoch: 117. Loss: 1.5148. Acc.: 67.21%\n",
      "Epoch: 120. Loss: 1.4777. Acc.: 67.21%\n",
      "Epoch: 123. Loss: 1.5258. Acc.: 67.21%\n",
      "Epoch: 126. Loss: 1.5071. Acc.: 67.21%\n",
      "Epoch: 129. Loss: 1.4548. Acc.: 67.21%\n",
      "Epoch: 132. Loss: 1.5216. Acc.: 67.75%\n",
      "Epoch: 135. Loss: 1.7114. Acc.: 67.75%\n",
      "Epoch: 138. Loss: 1.5302. Acc.: 67.75%\n",
      "Epoch: 141. Loss: 1.5354. Acc.: 67.75%\n",
      "Epoch: 144. Loss: 1.5511. Acc.: 67.75%\n",
      "Epoch: 147. Loss: 1.6345. Acc.: 67.75%\n",
      "Epoch: 150. Loss: 1.6269. Acc.: 67.75%\n",
      "Epoch: 153. Loss: 1.6261. Acc.: 67.75%\n",
      "Epoch: 156. Loss: 1.5483. Acc.: 67.75%\n",
      "Epoch: 159. Loss: 1.5567. Acc.: 67.75%\n",
      "Epoch: 162. Loss: 1.4783. Acc.: 78.92%\n",
      "Epoch: 165. Loss: 1.5025. Acc.: 79.28%\n",
      "Epoch: 168. Loss: 1.4571. Acc.: 79.64%\n",
      "Epoch: 171. Loss: 1.5363. Acc.: 79.82%\n",
      "Epoch: 174. Loss: 1.5097. Acc.: 79.82%\n",
      "Epoch: 177. Loss: 1.4568. Acc.: 79.82%\n",
      "Epoch: 180. Loss: 1.5531. Acc.: 79.82%\n",
      "Epoch: 183. Loss: 1.4313. Acc.: 79.82%\n",
      "Epoch: 186. Loss: 1.4362. Acc.: 79.82%\n",
      "Epoch: 189. Loss: 1.5054. Acc.: 79.82%\n",
      "Epoch: 192. Loss: 1.4587. Acc.: 79.82%\n",
      "Epoch: 195. Loss: 1.4447. Acc.: 79.82%\n",
      "Epoch: 198. Loss: 1.5288. Acc.: 79.82%\n",
      "Epoch: 201. Loss: 1.5039. Acc.: 79.82%\n",
      "Epoch: 204. Loss: 1.4578. Acc.: 79.82%\n",
      "Epoch: 207. Loss: 1.4343. Acc.: 79.82%\n",
      "Epoch: 210. Loss: 1.4429. Acc.: 79.82%\n",
      "Epoch: 213. Loss: 1.4806. Acc.: 79.82%\n",
      "Epoch: 216. Loss: 1.5257. Acc.: 79.82%\n",
      "Epoch: 219. Loss: 1.4654. Acc.: 79.82%\n",
      "Epoch: 222. Loss: 1.4494. Acc.: 79.82%\n",
      "Epoch: 225. Loss: 1.4581. Acc.: 79.82%\n",
      "Epoch: 228. Loss: 1.4347. Acc.: 79.82%\n",
      "Epoch: 231. Loss: 1.4422. Acc.: 79.82%\n",
      "Epoch: 234. Loss: 1.5869. Acc.: 79.82%\n",
      "Epoch: 237. Loss: 1.4659. Acc.: 79.82%\n",
      "Epoch: 240. Loss: 1.4337. Acc.: 79.82%\n",
      "Epoch: 243. Loss: 1.4975. Acc.: 79.82%\n",
      "Epoch: 246. Loss: 1.5184. Acc.: 79.82%\n",
      "Epoch: 249. Loss: 1.5103. Acc.: 79.82%\n",
      "Epoch: 252. Loss: 1.4346. Acc.: 79.82%\n",
      "Epoch: 255. Loss: 1.4499. Acc.: 79.82%\n",
      "Epoch: 258. Loss: 1.4797. Acc.: 79.82%\n",
      "Epoch: 261. Loss: 1.4501. Acc.: 79.82%\n",
      "Epoch: 264. Loss: 1.4727. Acc.: 79.82%\n",
      "Epoch: 267. Loss: 1.4954. Acc.: 79.82%\n",
      "Epoch: 270. Loss: 1.4874. Acc.: 79.82%\n",
      "Epoch: 273. Loss: 1.4348. Acc.: 79.82%\n",
      "Epoch: 276. Loss: 1.4570. Acc.: 79.82%\n",
      "Epoch: 279. Loss: 1.4666. Acc.: 79.82%\n",
      "Epoch: 282. Loss: 1.4872. Acc.: 79.82%\n",
      "Epoch: 285. Loss: 1.4190. Acc.: 79.82%\n",
      "Epoch: 288. Loss: 1.4574. Acc.: 79.82%\n",
      "Epoch: 291. Loss: 1.4347. Acc.: 79.82%\n",
      "Epoch: 294. Loss: 1.4422. Acc.: 79.82%\n",
      "Epoch: 297. Loss: 1.4793. Acc.: 79.82%\n",
      "Epoch: 300. Loss: 1.4882. Acc.: 79.82%\n"
     ]
    }
   ],
   "source": [
    "input_dim = np.shape(X)[1]\n",
    "output_dim = len(np.unique(yy))\n",
    "\n",
    "MLP = MLPClassifier(input_dim)\n",
    "model = MyEnsemble(MLP, output_dim)\n",
    "model = model.cuda()\n",
    "\n",
    "lr = 0.0001\n",
    "n_epochs = 300\n",
    "best_acc = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):   \n",
    "    for i, (x1_train, x2_train, x3_train, x4_train, x5_train, x6_train, x7_train, x8_train, x9_train, y_train) in enumerate(train_dl):\n",
    "        model.train()\n",
    "        x1_train, x2_train, x3_train, x4_train, x5_train, x6_train, x7_train, x8_train, x9_train, y_train = [t.cuda() for t in (x1_train, x2_train, x3_train, x4_train, x5_train, x6_train, x7_train, x8_train, x9_train, y_train)]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x1_train, x2_train, x3_train, x4_train, x5_train, x6_train, x7_train, x8_train, x9_train)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    Ncorrect, Nsample = 0, 0\n",
    "    for x1_val, x2_val, x3_val, x4_val, x5_val, x6_val, x7_val, x8_val, x9_val, y_val in valid_dl:\n",
    "        x1_val, x2_val, x3_val, x4_val, x5_val, x6_val, x7_val, x8_val, x9_val, y_val = [t.cuda() for t in (x1_val, x2_val, x3_val, x4_val, x5_val, x6_val, x7_val, x8_val, x9_val, y_val)]\n",
    "        out = model(x1_val, x2_val, x3_val, x4_val, x5_val, x6_val, x7_val, x8_val, x9_val)\n",
    "        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n",
    "        Nsample += y_val.size(0)\n",
    "        Ncorrect += (preds == y_val).sum().item()\n",
    "    \n",
    "    acc = Ncorrect / Nsample\n",
    "\n",
    "    if epoch % 3 == 0:\n",
    "        print(f'Epoch: {epoch:3d}. Loss: {loss.item():.4f}. Acc.: {acc:2.2%}')\n",
    "\n",
    "#     if acc > best_acc:\n",
    "#         best_acc = acc\n",
    "#         torch.save(model.state_dict(), 'best.pth')\n",
    "#         print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IWGJUHtJTe5d"
   },
   "source": [
    "**VISUALIZATION**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "jmK0yAE6BSiH",
    "outputId": "aff4ccd0-6a9f-439e-a744-5ab371baf9e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[285.,   4.],\n",
      "        [  2., 323.]])\n"
     ]
    }
   ],
   "source": [
    "nb_classes = output_dim\n",
    "\n",
    "confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "\n",
    "train_dl, valid_dl = create_dataloaders(train_ds, valid_ds, bs=len(y_valid))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x1_val, x2_val, x3_val, x4_val, x5_val, x6_val, x7_val, x8_val, x9_val, y_val in valid_dl:\n",
    "        x1_val, x2_val, x3_val, x4_val, x5_val, x6_val, x7_val, x8_val, x9_val, y_val = [t.cuda() for t in (x1_val, x2_val, x3_val, x4_val, x5_val, x6_val, x7_val, x8_val, x9_val, y_val)]\n",
    "        out = model(x1_val, x2_val, x3_val, x4_val, x5_val, x6_val, x7_val, x8_val, x9_val)\n",
    "        _, preds = torch.max(out, 1)\n",
    "        for t, p in zip(y_val.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 764
    },
    "colab_type": "code",
    "id": "809FPpO3Linu",
    "outputId": "8de95b0c-828d-4399-9144-b2611c599519"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(classification_report(y_valid, preds.tolist()))\n",
    "cm =confusion_matrix(y_valid, preds.tolist())  \n",
    "\n",
    "index = np.arange(0,output_dim)\n",
    "columns = np.arange(0,output_dim)\n",
    "# index = ['Healthy', 'Minor', 'Moderate', 'Severe']\n",
    "# columns = ['Healthy', 'Minor', 'Moderate', 'Severe']\n",
    "\n",
    "\n",
    "cm_df = pd.DataFrame(cm,columns,index)                      \n",
    "plt.figure(figsize=(12,9))  \n",
    "cm_df.index.name = 'Actual'\n",
    "cm_df.columns.name = 'Predicted'\n",
    "sns.set(font_scale=1.6)\n",
    "sns.heatmap(cm_df, annot=True, cmap= \"YlGnBu\", fmt='g')\n",
    "\n",
    "plt.savefig('severity.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MLP_raw_2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
